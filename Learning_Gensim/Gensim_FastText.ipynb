{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gensim_FastText.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C45-9BkXQeR-",
        "colab_type": "text"
      },
      "source": [
        "# Gensim Fast Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7CgcMPFQhDQ",
        "colab_type": "text"
      },
      "source": [
        "Here, we’ll learn to work with fastText library for training word-embedding models, saving & loading them and performing similarity operations & vector lookups analogous to Word2Vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4r3TEN2QjCY",
        "colab_type": "text"
      },
      "source": [
        "# When to use FastText?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4sv7t2CQ28P",
        "colab_type": "text"
      },
      "source": [
        "The main principle behind fastText is that the morphological structure of a word carries important information about the meaning of the word, which is not taken into account by traditional word embeddings, which train a unique word embedding for every individual word. \n",
        "\n",
        "This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSvKevSLQ7fE",
        "colab_type": "text"
      },
      "source": [
        "fastText attempts to solve this by treating each word as the aggregation of its subwords. \n",
        "\n",
        "For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRTEVoHHRGor",
        "colab_type": "text"
      },
      "source": [
        "According to a detailed comparison of Word2Vec and FastText in this notebook, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. \n",
        "\n",
        "Word2Vec slightly outperforms FastText on semantic tasks though. The differences grow smaller as the size of training corpus increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NDqnXapRMr9",
        "colab_type": "text"
      },
      "source": [
        "fastText can be used to obtain vectors for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6h5dqEmRPWO",
        "colab_type": "text"
      },
      "source": [
        "# Training models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLlUWV5MTw1a",
        "colab_type": "text"
      },
      "source": [
        "For the following examples, we’ll use the Lee Corpus (which you already have if you’ve installed gensim) for training our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niPNVsnLQAiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "from gensim.models.fasttext import FastText as FT_gensim\n",
        "from gensim.test.utils import datapath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfuM2ZR3Tyzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set file names for train and test data\n",
        "corpus_file = datapath('lee_background.cor')\n",
        "\n",
        "model = FT_gensim(size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJwM1QH4T2bk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "36174af6-b47d-4574-a65d-af33ca88e569"
      },
      "source": [
        "# build the vocabulary\n",
        "model.build_vocab(corpus_file=corpus_file)\n",
        "\n",
        "# train the model\n",
        "model.train(\n",
        "    corpus_file=corpus_file, epochs=model.epochs,\n",
        "    total_examples=model.corpus_count, total_words=model.corpus_total_words\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PX78alsT4y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "40a4c2b1-174b-4c96-fff9-ba7b45bde5e9"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<gensim.models.fasttext.FastText object at 0x7f5d4d3c2b38>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L4gFkEvUHlI",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n",
        "\n",
        "    model: Training architecture. Allowed values: cbow, skipgram (Default cbow)\n",
        "\n",
        "    size: Size of embeddings to be learnt (Default 100)\n",
        "\n",
        "    alpha: Initial learning rate (Default 0.025)\n",
        "\n",
        "    window: Context window size (Default 5)\n",
        "\n",
        "    min_count: Ignore words with number of occurrences below this (Default 5)\n",
        "\n",
        "    loss: Training objective. Allowed values: ns, hs, softmax (Default ns)\n",
        "\n",
        "    sample: Threshold for downsampling higher-frequency words (Default 0.001)\n",
        "\n",
        "    negative: Number of negative words to sample, for ns (Default 5)\n",
        "\n",
        "    iter: Number of epochs (Default 5)\n",
        "\n",
        "    sorted_vocab: Sort vocab by descending frequency (Default 1)\n",
        "\n",
        "    threads: Number of threads to use (Default 12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEAXbDU4ULM_",
        "colab_type": "text"
      },
      "source": [
        "In addition, FastText has three additional parameters:\n",
        "\n",
        "    min_n: min length of char ngrams (Default 3)\n",
        "\n",
        "    max_n: max length of char ngrams (Default 6)\n",
        "\n",
        "    bucket: number of buckets used for hashing ngrams (Default 2000000)\n",
        "\n",
        "Parameters min_n and max_n control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. \n",
        "\n",
        "If max_n is set to 0, or to be lesser than min_n, no character ngrams are used, and the model effectively reduces to Word2Vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRItnnrSUaSV",
        "colab_type": "text"
      },
      "source": [
        "To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the Fowler-Noll-Vo hashing function (FNV-1a variant) is employed.\n",
        "\n",
        "Note: As in the case of Word2Vec, you can continue to train your model while using Gensim’s native implementation of fastText."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YETbjztwUkcm",
        "colab_type": "text"
      },
      "source": [
        "# Saving/loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNsg2_wDVz5G",
        "colab_type": "text"
      },
      "source": [
        "Models can be saved and loaded via the load and save methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNBBfcmT6qL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "ddfcc3db-761c-4a78-82c3-7893e43cec6e"
      },
      "source": [
        "# saving a model trained via Gensim's fastText implementation\n",
        "import tempfile\n",
        "import os\n",
        "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
        "    model.save(tmp.name, separately=[])\n",
        "\n",
        "loaded_model = FT_gensim.load(tmp.name)\n",
        "print(loaded_model)\n",
        "\n",
        "os.unlink(tmp.name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<gensim.models.fasttext.FastText object at 0x7f5d4d3012e8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81Eb3MLlWBF3",
        "colab_type": "text"
      },
      "source": [
        "The save_word2vec_method causes the vectors for ngrams to be lost. As a result, a model loaded in this way will behave as a regular word2vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygvofspWH0l",
        "colab_type": "text"
      },
      "source": [
        "# Word vector lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxPJhVt1WM1z",
        "colab_type": "text"
      },
      "source": [
        "Note: Operations like word vector lookups and similarity queries can be performed in exactly the same manner for both the implementations of fastText so they have been demonstrated using only the native fastText implementation here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA4YOh0SWZkQ",
        "colab_type": "text"
      },
      "source": [
        "FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URHtrDxSV_EU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1eb91385-831c-44fd-fc35-aa9bd0fb6e05"
      },
      "source": [
        "print('night' in model.wv.vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG-L-ZxfWa8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4babd2c2-68b1-4795-c881-e0104b1479d9"
      },
      "source": [
        "print('nights' in model.wv.vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qs90f4sWcbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model['night'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6hnILYjWd8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model['nights'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RloXu4Wqy4",
        "colab_type": "text"
      },
      "source": [
        "The in operation works slightly differently from the original word2vec. It tests whether a vector for the given word exists or not, not whether the word is present in the word vocabulary. \n",
        "\n",
        "To test whether a word is present in the training word vocabulary -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEnxvAE-WgYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "624a68a9-7bdc-443e-c62b-fb504d83c991"
      },
      "source": [
        "print(\"word\" in model.wv.vocab)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YookZ1H9WyDU",
        "colab_type": "text"
      },
      "source": [
        "Tests if vector present for word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288rb9KyWvRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "b03cf79d-01d9-485b-8ac2-ec0edcc7ee25"
      },
      "source": [
        "print(\"word\" in model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zWMThdQXApe",
        "colab_type": "text"
      },
      "source": [
        "# Similarity operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylVjybf4aLcL",
        "colab_type": "text"
      },
      "source": [
        "Similarity operations work the same way as word2vec. Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5awmwRW1Mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7c76f22e-6196-45cd-9182-fa9c389c88e8"
      },
      "source": [
        "print(\"nights\" in model.wv.vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1_QS_w7aMmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c1cf77ea-7b1f-45ad-ddbe-f618e01d6126"
      },
      "source": [
        "print(\"night\" in model.wv.vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXQLofscaNdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "374af759-354e-419c-d561-75f1dfd52ef1"
      },
      "source": [
        "print(model.similarity(\"night\", \"nights\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.99999255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImnU8ccaOXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.accuracy(questions=datapath('questions-words.txt')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWa6n7tfkGJl",
        "colab_type": "text"
      },
      "source": [
        "# Word Movers distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiL8ZIGWkJII",
        "colab_type": "text"
      },
      "source": [
        "Let’s start with two sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHxHHx7Fj_7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
        "sentence_president = 'The president greets the press in Chicago'.lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aog6FYAYkPrv",
        "colab_type": "text"
      },
      "source": [
        "Remove their stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIpKpU2fkX2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlXjd6W0kWYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f0495985-9028-48ab-b39b-f6ef8d0a97fd"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_CJctRqkMg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
        "sentence_president = [w for w in sentence_president if w not in stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7DrgpIxkhmZ",
        "colab_type": "text"
      },
      "source": [
        "Compute WMD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nJBc02GkSpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "c0863ef3-0fdf-4daf-db69-f81549ec7a95"
      },
      "source": [
        "distance = model.wmdistance(sentence_obama, sentence_president)\n",
        "print(distance)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1198024487968088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}