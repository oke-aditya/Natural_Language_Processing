{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gensim_Word2Vecipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snb3VJBEoHJl",
        "colab_type": "text"
      },
      "source": [
        "# Using GenSim and Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAw7dtK6oUGs",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. \n",
        "\n",
        "The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. \n",
        "\n",
        "For example, strong and powerful would be close together and strong and Paris would be relatively far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAS3l07WoZiT",
        "colab_type": "text"
      },
      "source": [
        "The are two versions of this model and Word2Vec class implements them both:\n",
        "\n",
        "Skip-grams (SG)\n",
        "\n",
        "Continuous-bag-of-words (CBOW)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQzFuqLrpOS5",
        "colab_type": "text"
      },
      "source": [
        "The Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual one-hot encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRqcCPZpQZM",
        "colab_type": "text"
      },
      "source": [
        "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BVfFh_QpS-H",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LgVJT2opVro",
        "colab_type": "text"
      },
      "source": [
        "To see what Word2Vec can do, let’s download a pre-trained model and play around with it. We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases. Such a model can take hours to train, but since it’s already available, downloading and loading it with Gensim takes minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baWEwgAatFHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTPviqPIn76W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-oZ1pxUpcAH",
        "colab_type": "code",
        "outputId": "9937aa59-69b3-4fc0-a296-f5b59489f282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=======================---------------------------] 46.4% 771.5/1662.8MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqRsTGFtp3U7",
        "colab_type": "text"
      },
      "source": [
        "A common operation is to retrieve the vocabulary of a model. That is trivial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG8T1lZCpc7l",
        "colab_type": "code",
        "outputId": "89a14608-f540-4996-99e9-28e8663545dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "for i, word in enumerate(wv.vocab):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print(i, \"word = {}\".format(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 word = </s>\n",
            "1 word = in\n",
            "2 word = for\n",
            "3 word = that\n",
            "4 word = is\n",
            "5 word = on\n",
            "6 word = ##\n",
            "7 word = The\n",
            "8 word = with\n",
            "9 word = said\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ56sohdq1W9",
        "colab_type": "text"
      },
      "source": [
        "We can easily obtain vectors for terms the model is familiar with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0s9YPTGqxPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_king = wv['king']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOS7fPSWq2zD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vec_king)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gOBCFuDrHar",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, the model is unable to infer vectors for unfamiliar words. This is one limitation of Word2Vec: if this limitation matters to you, check out the FastText model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT4yiOParEHj",
        "colab_type": "code",
        "outputId": "a5d973c7-6254-406c-c80a-e42f1a2d20aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "try:\n",
        "    vec_cameroon = wv['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word 'cameroon' does not appear in this model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HPEuU7ZraVn",
        "colab_type": "text"
      },
      "source": [
        "Moving on, Word2Vec supports several word similarity tasks out of the box. You can see how the similarity intuitively decreases as the words get less and less similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIZLAXdArYDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXt2u2E8rfnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFc-yuUcsllp",
        "colab_type": "text"
      },
      "source": [
        "Print the 5 most similar words to “car” or “minivan”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRm7SA3Wsey2",
        "colab_type": "code",
        "outputId": "1b4a6b8d-8f7f-49ff-f6e7-3ad58f4ee973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(wv.most_similar(positive=[\"car\", \"minivan\"], topn=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('SUV', 0.853219211101532), ('vehicle', 0.8175784349441528), ('pickup_truck', 0.7763689160346985), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.756571888923645)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh-h2yQLs7_o",
        "colab_type": "text"
      },
      "source": [
        "Which of the below does not belong in the sequence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAmpehbsyRP",
        "colab_type": "code",
        "outputId": "d201c447-1b94-479c-c521-fc0c0f237474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtICLXGKuDET",
        "colab_type": "text"
      },
      "source": [
        "Find a similar word to given word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOzflbzps_2H",
        "colab_type": "code",
        "outputId": "b36e3184-8f04-49d8-b8c9-ff0c67ebc92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(wv.similar_by_word(word=\"car\", topn=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962820053101), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulR3f7LzuE-O",
        "colab_type": "text"
      },
      "source": [
        "Compute Cosine similarity between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3TKgRMJtry6",
        "colab_type": "code",
        "outputId": "cfd5b4a4-797b-47b4-f5ef-940d0534d41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(wv.cosine_similarities(wv[\"car\"], [wv[\"vehicle\"]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.78210956]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z0YrPwkuIsj",
        "colab_type": "text"
      },
      "source": [
        "Get all words that are closer to w1 than w2 is to w1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfHJTZ9ft7EO",
        "colab_type": "code",
        "outputId": "3ab7561a-3190-40b2-e150-7a8a2f63e036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(wv.words_closer_than(\"car\", \"SUV\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['vehicle', 'cars']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIXIV5vpugtz",
        "colab_type": "text"
      },
      "source": [
        "# Training Your Own Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1XNoqP-uksA",
        "colab_type": "text"
      },
      "source": [
        "To start, you’ll need some data for training the model. For the following examples, we’ll use the Lee Corpus (which you already have if you’ve installed gensim).\n",
        "\n",
        "This corpus is small enough to fit entirely in memory, but we’ll implement a memory-friendly iterator that reads it line-by-line to demonstrate how you would handle a larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ks-myS5uZEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffKsSbzWuoI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCorpus(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjCZNUjuuvNt",
        "colab_type": "text"
      },
      "source": [
        "If we wanted to do any custom preprocessing, e.g. decode a non-standard encoding, lowercase, remove numbers, extract named entities… All of this can be done inside the MyCorpus iterator and word2vec doesn’t need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
        "\n",
        "Let’s go ahead and train a model on our corpus. Don’t worry about the training parameters much for now, we’ll revisit them later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPk6k6GqupK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j33OrhHav7FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1lvhjQUv_vM",
        "colab_type": "text"
      },
      "source": [
        "Once we have our model, we can use it in the same way as in the demo above.\n",
        "\n",
        "The main part of the model is model.wv, where “wv” stands for “word vectors”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVHSqTGdv9PB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_king = model.wv['king']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Q-jEtDwDIS",
        "colab_type": "text"
      },
      "source": [
        "Retrieving the vocabulary works the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldfQharGwA41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(model.wv.vocab):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhkgfjwDw347",
        "colab_type": "text"
      },
      "source": [
        "# Storing and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHpP2nhAw7sx",
        "colab_type": "text"
      },
      "source": [
        "You’ll notice that training non-trivial models can take time. Once you’ve trained your model and it works as expected, you can save it to disk. That way, you don’t have to spend time training it all over again later.\n",
        "\n",
        "You can store/load models using the standard gensim methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41eLKdTLwFUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tempfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpbxPA0Nw9z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    #\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc.\n",
        "    #\n",
        "    # To load a saved model:\n",
        "    #\n",
        "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Jdxt99xIHi",
        "colab_type": "text"
      },
      "source": [
        "which uses pickle internally, optionally mmap‘ing the model’s internal large NumPy matrices into virtual memory directly from disk files, for inter-process memory sharing.\n",
        "\n",
        "In addition, you can load models created by the original C tool, both using its text and binary formats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beh82pExC14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
        "# using gzipped/bz2 input works too, no need to unzip\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC44DgEtxSGB",
        "colab_type": "text"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODv1VcOxT7I",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec accepts several parameters that affect both training speed and quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSuhpfs3yA3A",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# min_count\n",
        "\n",
        "min_count is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n",
        "\n",
        "default value of min_count=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcWbwsbgyCW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences, min_count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kthlO59myHIa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# size\n",
        "\n",
        "size is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
        "\n",
        "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
        "\n",
        "default value of size=100\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFx76V1LyIwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences, size=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9olFIryLg_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# workers\n",
        "\n",
        "workers , the last of the major parameters (full list here) is for training parallelization, to speed up training:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mumfMux5xb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "default value of workers=3 (tutorial says 1...)\n",
        "\n",
        "\n",
        "\n",
        "The workers parameter only has an effect if you have Cython installed. Without Cython, you’ll only be able to use one core because of the GIL (and word2vec training will be miserably slow).\n",
        "Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDduRlFyOfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIMRqAywyaF8",
        "colab_type": "text"
      },
      "source": [
        "# Memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snOmUjLx4NPR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "At its core, word2vec model parameters are stored as matrices (NumPy arrays). Each array is #vocabulary (controlled by min_count parameter) times #size (size parameter) of floats (single precision aka 4 bytes).\n",
        "\n",
        "Three such matrices are held in RAM (work is underway to reduce that number to two, or even one). So if your input contains 100,000 unique words, and you asked for layer size=200, the model will require approx. 100,000*200*4*3 bytes = ~229MB.\n",
        "\n",
        "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would take a few megabytes), but unless your words are extremely loooong strings, memory footprint will be dominated by the three matrices above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkeL-o_O0Uq6",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3KIccQ0XF7",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec training is an unsupervised task, there’s no good way to objectively evaluate the result. Evaluation depends on your end application.\n",
        "\n",
        "Google has released their testing set of about 20,000 syntactic and semantic test examples, following the “A is to B as C is to D” task. It is provided in the ‘datasets’ folder.\n",
        "\n",
        "For example a syntactic analogy of comparative type is bad:worse;good:?. There are total of 9 types of syntactic comparisons in the dataset like plural nouns and nouns of opposite meaning.\n",
        "\n",
        "The semantic questions contain five types of semantic analogies, such as capital cities (Paris:France;Tokyo:?) or family members (brother:sister;dad:?).\n",
        "\n",
        "Gensim supports the same evaluation set, in exactly the same format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znO5Rfs8xJ2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.accuracy('./datasets/questions-words.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF_fSq9218c4",
        "colab_type": "text"
      },
      "source": [
        "This accuracy takes an optional parameter restrict_vocab which limits which test examples are to be considered.\n",
        "\n",
        "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
        "\n",
        "By default it uses an academic dataset WS-353 but one can create a dataset specific to your business based on it. It contains word pairs together with human-assigned similarity judgments. It measures the relatedness or co-occurrence of two words. For example, ‘coast’ and ‘shore’ are very similar as they appear in the same context. At the same time ‘clothes’ and ‘closet’ are less similar because they are related but not interchangeable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J36qmCAq2COF",
        "colab_type": "text"
      },
      "source": [
        "# Online training / Resuming training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtiMQEgt2Ubr",
        "colab_type": "text"
      },
      "source": [
        "Advanced users can load a model and continue training it with more sentences and new vocabulary words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8sYG6cT0gJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec.load(temporary_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BP2J4Qr2Vv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more_sentences = [\n",
        "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
        "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
        "]\n",
        "model.build_vocab(more_sentences, update=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0scT66us2Xmy",
        "colab_type": "code",
        "outputId": "9564c538-0a89-4d34-f458-bca28592898a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqwy3ge2Yzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning up temporary file\n",
        "import os\n",
        "os.remove(temporary_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxC616ej2eF3",
        "colab_type": "text"
      },
      "source": [
        "You may need to tweak the total_words parameter to train(), depending on what learning rate decay you want to simulate.\n",
        "\n",
        "Note that it’s not possible to resume training with models generated by the C tool, KeyedVectors.load_word2vec_format(). You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPC_vZm34FQL",
        "colab_type": "text"
      },
      "source": [
        "# Training Loss Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9fABF34HXZ",
        "colab_type": "text"
      },
      "source": [
        "The parameter compute_loss can be used to toggle computation of loss while training the Word2Vec model. The computed loss is stored in the model attribute running_training_loss and can be retrieved using the function get_latest_training_loss as follows :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMDnaX-62aKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiating and training the Word2Vec model\n",
        "model_with_loss = gensim.models.Word2Vec(\n",
        "    sentences,\n",
        "    min_count=1,\n",
        "    compute_loss=True,\n",
        "    hs=0,\n",
        "    sg=1,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DXKb_iV4PyJ",
        "colab_type": "code",
        "outputId": "da09ad32-6683-49d1-c8e8-97926f58bd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# getting the training loss value\n",
        "training_loss = model_with_loss.get_latest_training_loss()\n",
        "print(training_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1636602.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u67Z-5lf4dgW",
        "colab_type": "text"
      },
      "source": [
        "# Benchmarks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaxZW0bX4o12",
        "colab_type": "text"
      },
      "source": [
        "Let’s run some benchmarks to see effect of the training loss computation code on training time.\n",
        "\n",
        "We’ll use the following data for the benchmarks:\n",
        "\n",
        "Lee Background corpus: included in gensim’s test data\n",
        "\n",
        "Text8 corpus. To demonstrate the effect of corpus size, we’ll look at the first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulAdGly64R6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "import gensim.models.word2vec\n",
        "import gensim.downloader as api\n",
        "import smart_open\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r7rAoM04ql4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def head(path, size):\n",
        "    with smart_open.open(path) as fin:\n",
        "        return io.StringIO(fin.read(size))\n",
        "\n",
        "\n",
        "def generate_input_data():\n",
        "    lee_path = datapath('lee_background.cor')\n",
        "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
        "    ls.name = '25kB'\n",
        "    yield ls\n",
        "\n",
        "    text8_path = api.load('text8').fn\n",
        "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
        "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
        "    for l, s in zip(labels, sizes):\n",
        "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
        "        ls.name = l\n",
        "        yield ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIWLX1PJ4t3O",
        "colab_type": "code",
        "outputId": "d50ab5b4-552d-4fda-e3fe-8ec0fcf5f3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "input_data = list(generate_input_data())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xklQ-6-a40Ts",
        "colab_type": "text"
      },
      "source": [
        "We now compare the training time taken for different combinations of input data and model training parameters like hs and sg.\n",
        "\n",
        "For each combination, we repeat the test several times to obtain the mean and standard deviation of the test duration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2aSG3G4vi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temporarily reduce logging verbosity\n",
        "logging.root.level = logging.ERROR\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_time_values = []\n",
        "seed_val = 42\n",
        "sg_values = [0, 1]\n",
        "hs_values = [0, 1]\n",
        "\n",
        "fast = True\n",
        "if fast:\n",
        "    input_data_subset = input_data[:3]\n",
        "else:\n",
        "    input_data_subset = input_data\n",
        "\n",
        "\n",
        "for data in input_data_subset:\n",
        "    for sg_val in sg_values:\n",
        "        for hs_val in hs_values:\n",
        "            for loss_flag in [True, False]:\n",
        "                time_taken_list = []\n",
        "                for i in range(3):\n",
        "                    start_time = time.time()\n",
        "                    w2v_model = gensim.models.Word2Vec(\n",
        "                        data,\n",
        "                        compute_loss=loss_flag,\n",
        "                        sg=sg_val,\n",
        "                        hs=hs_val,\n",
        "                        seed=seed_val,\n",
        "                    )\n",
        "                    time_taken_list.append(time.time() - start_time)\n",
        "\n",
        "                time_taken_list = np.array(time_taken_list)\n",
        "                time_mean = np.mean(time_taken_list)\n",
        "                time_std = np.std(time_taken_list)\n",
        "\n",
        "                model_result = {\n",
        "                    'train_data': data.name,\n",
        "                    'compute_loss': loss_flag,\n",
        "                    'sg': sg_val,\n",
        "                    'hs': hs_val,\n",
        "                    'train_time_mean': time_mean,\n",
        "                    'train_time_std': time_std,\n",
        "                }\n",
        "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
        "                train_time_values.append(model_result)\n",
        "\n",
        "train_times_table = pd.DataFrame(train_time_values)\n",
        "train_times_table = train_times_table.sort_values(\n",
        "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
        "    ascending=[False, False, True, False],\n",
        ")\n",
        "print(train_times_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE2Yfquw2zHg",
        "colab_type": "text"
      },
      "source": [
        "# Production Pipeline Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLCapfO6oSG",
        "colab_type": "text"
      },
      "source": [
        "Suppose, we still want more performance improvement in production.\n",
        "\n",
        "One good way is to cache all the similar words in a dictionary.\n",
        "\n",
        "So that next time when we get the similar query word, we’ll search it first in the dict.\n",
        "\n",
        "And if it’s a hit then we will show the result directly from the dictionary.\n",
        "\n",
        "otherwise we will query the word and then cache it so that it doesn’t miss next time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgZ5lqnG63fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "words = ['voted', 'few', 'their', 'around']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi6M3AUB69OL",
        "colab_type": "text"
      },
      "source": [
        "without caching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrX3Kym2196",
        "colab_type": "code",
        "outputId": "c3aab955-7172-4ac8-a4ef-94a721475611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "start = time.time()\n",
        "for word in words:\n",
        "    result = wv.most_similar(word)\n",
        "    print(result)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fcda2769ca71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZhBiFeq6qCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with caching\n",
        "start = time.time()\n",
        "for word in words:\n",
        "    if 'voted' in most_similars_precalc:\n",
        "        result = most_similars_precalc[word]\n",
        "        print(result)\n",
        "    else:\n",
        "        result = wv.most_similar(word)\n",
        "        most_similars_precalc[word] = result\n",
        "        print(result)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}