# -*- coding: utf-8 -*-
"""Similarity_Queries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oprCjSgme-w3kC16jdxV6hA1DJA9RUxc

# Get the Data
"""

documents = [
    "Human machine interface for lab abc computer applications",
    "A survey of user opinion of computer system response time",
    "The EPS user interface management system",
    "System and human system engineering testing of EPS",
    "Relation of user perceived response time to error measurement",
    "The generation of random binary unordered trees",
    "The intersection graph of paths in trees",
    "Graph minors IV Widths of trees and well quasi ordering",
    "Graph minors A survey",
]

"""This is a tiny corpus of nine documents, each consisting of only a single sentence.

First, let’s tokenize the documents, remove common words (using a toy stoplist) as well as words that only appear once in the corpus:
"""

from pprint import pprint 
from collections import defaultdict
from gensim import corpora

stoplist = set('for a of the and in'.split())
print(stoplist)

texts = []
for document in documents:
    textl = []
    for word in document.lower().split():
        if word not in stoplist:
            textl.append(word)
    
    texts.append(textl)

pprint(texts)

# remove words that appear only once
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1

texts_final = []

for text in texts:
    texts_doc = []
    for token in text:
        if frequency[token] > 1:
            texts_doc.append(token)
    texts_final.append(texts_doc)

pprint(texts_final)

dictionary = corpora.Dictionary(texts_final)

corpus = []
for text in texts_final:
    corpus.append(dictionary.doc2bow(text))

pprint(corpus)

"""# Similarity interface

In the previous tutorials on Corpora and Vector Spaces and Topics and Transformations, we covered what it means to create a corpus in the Vector Space Model and how to transform it between different vector spaces. 

A common reason for such a charade is that we want to determine similarity between pairs of documents, or the similarity between a specific document and a set of other documents (such as a user query vs. indexed documents).

To show how this can be done in gensim, let us consider the same corpus as in the previous examples (which really originally comes from Deerwester et al.’s “Indexing by Latent Semantic Analysis” seminal 1990 article). To follow Deerwester’s example, we first use this tiny corpus to define a 2-dimensional LSI space:
"""

from gensim import models

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

"""For the purposes of this tutorial, there are only two things you need to know about LSI. First, it’s just another transformation: it transforms vectors from one space to another. 

Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics. Our LSI space is two-dimensional (num_topics = 2) so there are two topics, but this is arbitrary. If you’re interested, you can read more about LSI here: Latent Semantic Indexing:

For the purposes of this tutorial, there are only two things you need to know about LSI. First, it’s just another transformation: it transforms vectors from one space to another. Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics. 

Our LSI space is two-dimensional (num_topics = 2) so there are two topics, but this is arbitrary. If you’re interested, you can read more about LSI here: Latent Semantic Indexing:
"""

doc = "Human computer interaction"
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]  # convert the query to LSI space
print(vec_lsi)

"""In addition, we will be considering cosine similarity to determine the similarity of two vectors. 

Cosine similarity is a standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity measures may be more appropriate.

# Initializing query structures

To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. 

In our case, they are the same nine documents used for training LSI, converted to 2-D LSA space. But that’s only incidental, we might also be indexing a different corpus altogether.
"""

from gensim import similarities

index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it

"""The class similarities.MatrixSimilarity is only appropriate when the whole set of vectors fits into memory. For example, a corpus of one million documents would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.

Without 2GB of free RAM, you would need to use the similarities.Similarity class. This class operates in fixed memory, by splitting the index across multiple files on disk, called shards. It uses similarities.MatrixSimilarity and similarities.SparseMatrixSimilarity internally, so it is still fast, although slightly more complex.

This is true for all similarity indexing classes (similarities.Similarity, similarities.MatrixSimilarity and similarities.SparseMatrixSimilarity). Also in the following, index can be an object of any of these. When in doubt, use similarities.Similarity, as it is the most scalable version, and it also supports adding more documents to the index later.

# Performing queries

To obtain similarities of our query document against the nine indexed documents:
"""

sims = index[vec_lsi]  # perform a similarity query against the corpus
print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples

"""Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a score of 0.99809301 etc.

With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query “Human computer interaction”:
"""

sims = sorted(enumerate(sims), key=lambda item: -item[1])
for i, s in enumerate(sims):
    print(s, documents[i])