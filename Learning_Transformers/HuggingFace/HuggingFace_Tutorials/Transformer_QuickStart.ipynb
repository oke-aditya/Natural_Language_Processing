{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_QuickStart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q61RPB9uHLjC",
        "colab_type": "text"
      },
      "source": [
        "# QuickStart to HuggingFace Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s3r4dWvHiPb",
        "colab_type": "text"
      },
      "source": [
        "## What is transformers and how it is structured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvkOeJT7HRFw",
        "colab_type": "text"
      },
      "source": [
        "https://huggingface.co/transformers/v2.5.0/quickstart.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wfcvivHUC9",
        "colab_type": "text"
      },
      "source": [
        "Transformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnJvxrZ4HXjD",
        "colab_type": "text"
      },
      "source": [
        "The library is build around three type of classes for each models:\n",
        "\n",
        "1. model classes which are PyTorch models (torch.nn.Modules) of the 8 models architectures currently provided in the library, e.g. BertModel\n",
        "\n",
        "2. configuration classes which store all the parameters required to build a model, e.g. BertConfig. You don’t always need to instantiate these your-self, in particular if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)\n",
        "\n",
        "3. tokenizer classes which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model, e.g. BertTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqqtwyqHrc4",
        "colab_type": "text"
      },
      "source": [
        "All these classes can be instantiated from pretrained instances and saved locally using two methods:\n",
        "\n",
        "1. from_pretrained() let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed here) or stored locally (or on a server) by the user,\n",
        "\n",
        "2. save_pretrained() let you save a model/configuration/tokenizer locally so that it can be reloaded using from_pretrained().\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdbcVjP3H2EI",
        "colab_type": "text"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9Wt1heH3j-",
        "colab_type": "text"
      },
      "source": [
        "Use Pytorch 1.x and tensorflow 2.x\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFrw_jgBHFF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbOY_OD_IIbR",
        "colab_type": "text"
      },
      "source": [
        "# Quick Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anxR9X02I3dA",
        "colab_type": "text"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbsCrDMI7Vo",
        "colab_type": "text"
      },
      "source": [
        "Let’s start by preparing a tokenized input (a list of token embeddings indices to be fed to Bert) from a text string using BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJVoLWgoIAlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wihychDILJqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = 8\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "# assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EecJL9yLtE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87b3b049-9fd0-40f5-ba23-2ee676dbeaf2"
      },
      "source": [
        "print(tokenized_text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmPJjAlLXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SleR2ibuLy2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "270a28c9-8cc7-47c6-d834-746142a5ddb6"
      },
      "source": [
        "print(tokens_tensor.shape)\n",
        "print(segments_tensors.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 14])\n",
            "torch.Size([1, 14])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJUAIso1JgJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "# This is IMPORTANT to have reproducible results during evaluation!\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXmm6RjIJmq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXippbIuJx6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    # See the models docstrings for the detail of the inputs\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    # Transformers models always output tuples.\n",
        "    # See the models docstrings for the detail of all the outputs\n",
        "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
        "    encoded_layers = outputs[0]\n",
        "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
        "# assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZoFW4PMLr9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ee10fb48-2a61-4c2a-c301-df045bef9285"
      },
      "source": [
        "print(encoded_layers.shape)\n",
        "print(model.config.hidden_size)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 14, 768])\n",
            "768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbjMNqYfLhk5",
        "colab_type": "text"
      },
      "source": [
        "And how to use BertForMaskedLM to predict a masked token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQHnjJuyK0yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCopUGjmOT1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR7DqjFvOSIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "# assert predicted_token == 'henson'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-zlEXRQLkMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e382888c-4a76-4e61-cf96-fa92ec7a7ba0"
      },
      "source": [
        "print(predicted_token)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "henson\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzxcSDmcMjdQ",
        "colab_type": "text"
      },
      "source": [
        "## Using OpenAI GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8fdywo5MrcU",
        "colab_type": "text"
      },
      "source": [
        "Here is a quick-start example using GPT2Tokenizer and GPT2LMHeadModel class with OpenAI’s pre-trained model to predict the next token from a text prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxMu8YUMtfQ",
        "colab_type": "text"
      },
      "source": [
        "First let’s prepare a tokenized input from our text string using GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDa4uIXkMmdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok1IImOcMwAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN11gHyHNHwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode a text inputs\n",
        "text = \"Who was Jim Henson ? Jim Henson was a\"\n",
        "indexed_tokens = tokenizer.encode(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ApHPsfNLi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f036d42-8316-42c4-d51f-33b91c6f0c59"
      },
      "source": [
        "print(indexed_tokens)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8241, 373, 5395, 367, 19069, 5633, 5395, 367, 19069, 373, 257]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPscG_dNNXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28fffffc-be06-4693-8c74-ac4141537f31"
      },
      "source": [
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "print(tokens_tensor.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G4ca49LNXZC",
        "colab_type": "text"
      },
      "source": [
        "Let’s see how to use GPT2LMHeadModel to generate the next token following our text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eei9whBZM93m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB5BEIs_NC31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "# This is IMPORTANT to have reproducible results during evaluation!\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqEXMeFDNcs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT9k5vB7NsbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1nV4df2Nv2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "d28d4975-48dc-48ce-c922-f329a45e6450"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ -38.7640,  -38.4434,  -42.0484,  ...,  -45.7126,  -43.8866,\n",
            "           -38.7747],\n",
            "         [-104.8652, -103.7108, -108.8487,  ..., -112.9846, -110.5398,\n",
            "          -107.1295],\n",
            "         [ -71.2187,  -70.2136,  -76.3859,  ...,  -84.0147,  -78.1190,\n",
            "           -73.4547],\n",
            "         ...,\n",
            "         [ -96.3705,  -98.9886, -102.8611,  ..., -110.6566, -103.5795,\n",
            "           -99.4158],\n",
            "         [-101.4872, -102.2246, -106.9355,  ..., -111.9820, -107.7468,\n",
            "          -105.1568],\n",
            "         [-111.4282, -111.0716, -115.7848,  ..., -121.6386, -117.4221,\n",
            "          -113.0788]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjeeb_JNx-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the predicted next sub-word (in our case, the word 'man')\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEXCvKNiN2Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTIEmBETN5Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9905ad1-1e6e-405a-fffe-b2d450c1508d"
      },
      "source": [
        "print(predicted_text)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Who was Jim Henson? Jim Henson was a man\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlLGqHiAOKNQ",
        "colab_type": "text"
      },
      "source": [
        "## Using the Past"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHM_fjRSOYQz",
        "colab_type": "text"
      },
      "source": [
        "GPT-2 as well as some other models (GPT, XLNet, Transfo-XL, CTRL) make use of a past or mems attribute which can be used to prevent re-computing the key/value pairs when using sequential decoding. It is useful when generating sequences as a big part of the attention mechanism benefits from previous computations.\n",
        "\n",
        "Here is a fully-working example using the past with GPT2LMHeadModel and argmax decoding (which should only be used as an example, as argmax decoding introduces a lot of repetition):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klNqym31OLP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLcOlEgdOd0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzrcSP4vOgHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated = tokenizer.encode(\"The Manhattan bridge\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Yev_69OlPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "921828d4-2e4e-4633-fbb5-03286f223283"
      },
      "source": [
        "print(generated)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[464, 13458, 7696]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8eUdM-hOmcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = torch.tensor([generated])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-4cdmZQOpqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "past = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQBoLN7oOrjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    print(i)\n",
        "    output, past = model(context, past=past)\n",
        "    token = torch.argmax(output[..., -1, :])\n",
        "\n",
        "    generated += [token.tolist()]\n",
        "    context = token.unsqueeze(0)\n",
        "\n",
        "sequence = tokenizer.decode(generated)\n",
        "print(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriTDpEuUOAe",
        "colab_type": "text"
      },
      "source": [
        "The model only requires a single token as input as all the previous tokens’ key/value pairs are contained in the past."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_SRAdYeUWP-",
        "colab_type": "text"
      },
      "source": [
        "## Model2Model example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnq9vMjqVJfI",
        "colab_type": "text"
      },
      "source": [
        "Encoder-decoder architectures require two tokenized inputs: one for the encoder and the other one for the decoder.\n",
        "\n",
        "Let’s assume that we want to use Model2Model for generative question answering, and start by tokenizing the question and answer that will be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNZ78rcVV3ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3UoGgp8VJUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, Model2Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udotSXiiSNP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Encode the input to the encoder (the question)\n",
        "question = \"Who was Jim Henson?\"\n",
        "encoded_question = tokenizer.encode(question)\n",
        "\n",
        "# Encode the input to the decoder (the answer)\n",
        "answer = \"Jim Henson was a puppeteer\"\n",
        "encoded_answer = tokenizer.encode(answer)\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "question_tensor = torch.tensor([encoded_question])\n",
        "answer_tensor = torch.tensor([encoded_answer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ENKgVDCXlT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88fe7977-db0b-4564-9a54-e42dc2affecb"
      },
      "source": [
        "print(encoded_question)\n",
        "print(encoded_answer)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2040, 2001, 3958, 27227, 1029, 102]\n",
            "[101, 3958, 27227, 2001, 1037, 13997, 11510, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU3DKdt_Xqxh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "25af509a-8aa8-4c99-9931-b637cf6147dd"
      },
      "source": [
        "print(question_tensor.shape)\n",
        "print(answer_tensor.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7])\n",
            "torch.Size([1, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yvns646Xfqh",
        "colab_type": "text"
      },
      "source": [
        "Let’s see how we can use Model2Model to get the value of the loss associated with this (question, answer) pair:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDeZ4I_iV_fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In order to compute the loss we need to provide language model\n",
        "# labels (the token ids that the model should have produced) to\n",
        "# the decoder.\n",
        "lm_labels =  encoded_answer\n",
        "labels_tensor = torch.tensor([lm_labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqExiiHkXiOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac8aaf5a-a615-4ead-f106-adfe356ff6b3"
      },
      "source": [
        "print(labels_tensor.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI-Fic3RYBqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = Model2Model.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAFHN5TNYtsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have a GPU, put everything on cuda\n",
        "question_tensor = question_tensor.to('cuda')\n",
        "answer_tensor = answer_tensor.to('cuda')\n",
        "labels_tensor = labels_tensor.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1eIdo-TZND-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    # See the models docstrings for the detail of the inputs\n",
        "    outputs = model(question_tensor, answer_tensor, decoder_lm_labels=labels_tensor)\n",
        "    # Transformers models always output tuples.\n",
        "    # See the models docstrings for the detail of all the outputs\n",
        "    # In our case, the first element is the value of the LM loss \n",
        "    lm_loss = outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk3HNV6uZRGN",
        "colab_type": "text"
      },
      "source": [
        "This loss can be used to fine-tune Model2Model on the question answering task. Assuming that we fine-tuned the model, let us now see how to generate an answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYqW_mYZRY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's re-use the previous question\n",
        "question = \"Who was Jim Henson?\"\n",
        "encoded_question = tokenizer.encode(question)\n",
        "question_tensor = torch.tensor([encoded_question])\n",
        "\n",
        "# This time we try to generate the answer, so we start with an empty sequence\n",
        "answer = \"[CLS]\"\n",
        "encoded_answer = tokenizer.encode(answer, add_special_tokens=False)\n",
        "answer_tensor = torch.tensor([encoded_answer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLZnIxOeZgAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = Model2Model.from_pretrained('fine-tuned-weights')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFW009obZbi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have a GPU, put everything on cuda\n",
        "question_tensor = question_tensor.to('cuda')\n",
        "answer_tensor = answer_tensor.to('cuda')\n",
        "model.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g8fRttOZd76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(question_tensor, answer_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# confirm we were able to predict 'jim'\n",
        "predicted_index = torch.argmax(predictions[0, -1]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULucwxkNZi2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(predicted_token)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}